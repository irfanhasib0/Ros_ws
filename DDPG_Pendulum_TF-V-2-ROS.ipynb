{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "env_to_use = 'Pendulum-v0'\n",
    "\n",
    "# hyperparameters\n",
    "gamma = 0.99\t\t\t\t# reward discount factor\n",
    "h1_actor = 8\t\t\t# hidden layer 1 size for the actor\n",
    "h2_actor = 8\t\t\t\t# hidden layer 2 size for the actor\n",
    "h3_actor = 8\t\t\t\t# hidden layer 3 size for the actor\n",
    "h1_critic = 8\t\t\t\t# hidden layer 1 size for the critic\n",
    "h2_critic = 8\t\t\t\t# hidden layer 2 size for the critic\n",
    "h3_critic = 8\t\t\t\t# hidden layer 3 size for the critic\n",
    "lr_actor = 1e-3\t\t\t\t# learning rate for the actor\n",
    "lr_critic = 1e-3\t\t\t# learning rate for the critic\n",
    "lr_decay = 1\t\t\t\t# learning rate decay (per episode)\n",
    "l2_reg_actor = 1e-6\t\t\t# L2 regularization factor for the actor\n",
    "l2_reg_critic = 1e-6\t\t# L2 regularization factor for the critic\n",
    "dropout_actor = 0\t\t\t# dropout rate for actor (0 = no dropout)\n",
    "dropout_critic = 0\t\t\t# dropout rate for critic (0 = no dropout)\n",
    "num_episodes = 150\t\t# number of episodes\n",
    "max_steps_ep = 10000\t# default max number of steps per episode (unless env has a lower hardcoded limit)\n",
    "tau = 1e-2\t\t\t\t# soft target update rate\n",
    "train_every = 1\t\t\t# number of steps to run the policy (and collect experience) before updating network weights\n",
    "replay_memory_capacity = int(1e5)\t# capacity of experience replay memory\n",
    "minibatch_size = 1024\t# size of minibatch from experience replay memory for updates\n",
    "initial_noise_scale = 0.1\t# scale of the exploration noise process (1.0 is the range of each action dimension)\n",
    "noise_decay = 0.99\t\t# decay rate (per episode) of the scale of the exploration noise process\n",
    "exploration_mu = 0.0\t# mu parameter for the exploration noise process: dXt = theta*(mu-Xt)*dt + sigma*dWt\n",
    "exploration_theta = 0.15 # theta parameter for the exploration noise process: dXt = theta*(mu-Xt)*dt + sigma*dWt\n",
    "exploration_sigma = 0.2\t# sigma parameter for the exploration noise process: dXt = theta*(mu-Xt\t)*dt + sigma*dWt\n",
    "\n",
    "# game parameters\n",
    "env = gym.make(env_to_use)\n",
    "state_dim = np.prod(np.array(env.observation_space.shape)) \t# Get total number of dimensions in state\n",
    "action_dim = np.prod(np.array(env.action_space.shape))\t\t# Assuming continuous action space\n",
    "\n",
    "# set seeds to 0\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# prepare monitorings\n",
    "#outdir = '/tmp/ddpg-agent-results'\n",
    "#env = wrappers.Monitor(env, outdir, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=replay_memory_capacity)\t\t\t# used for O(1) popleft() operation\n",
    "\n",
    "def add_to_memory(experience):\n",
    "\treplay_memory.append(experience)\n",
    "\n",
    "def sample_from_memory(minibatch_size):\n",
    "\treturn random.sample(replay_memory, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class ANN():\n",
    "    tf.reset_default_graph()\n",
    "    # placeholders\n",
    "    state_ph  =  tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "    action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "    reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "    is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None]) # indicators (go into target computation)\n",
    "    #is_training_ph = tf.placeholder(dtype=tf.bool, shape=()) # for dropout\n",
    "\n",
    "    # episode counter\n",
    "    episodes = tf.Variable(0.0, trainable=False, name='episodes')\n",
    "    episode_inc_op = episodes.assign_add(1)\n",
    "    \n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_net_value = ANN.generate_actor_network(self,trainable = True, reuse = False)\n",
    "\n",
    "        # slow target actor network\n",
    "        with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "            self.target_actor_net_value = tf.stop_gradient(ANN.generate_actor_network(self,trainable = False, reuse = False))\n",
    "\n",
    "        with tf.variable_scope('critic') as scope:\n",
    "            self.critic_net_value = ANN.generate_critic_network(self,trainable = True, reuse = False)\n",
    "            self.q_value_for_actor_net = ANN.generate_critic_network(self,trainable = True, reuse = True,mode=2)\n",
    "\n",
    "        # slow target critic\n",
    "        with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "            self.target_critic_net_value = tf.stop_gradient(ANN.generate_critic_network(self,trainable = False, reuse = False,mode=3))\n",
    "        # isolate vars for each network\n",
    "        \n",
    "        self.actor_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        self.target_actor_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_actor')\n",
    "        self.critic_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "        self.target_critic_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_critic')\n",
    "\n",
    "         # actor network\n",
    "    def predict_graph(self):\n",
    "        return self.actor_net_value\n",
    "    def generate_actor_network(self,trainable, reuse):\n",
    "        hidden = tf.layers.dense(ANN.state_ph, h1_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        hidden_2 = tf.layers.dense(hidden, h2_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        hidden_3 = tf.layers.dense(hidden_2, h3_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        actions_unscaled = tf.layers.dense(hidden_3, action_dim, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        actions = env.action_space.low + tf.nn.sigmoid(actions_unscaled)*(env.action_space.high - env.action_space.low) # bound the actions to the valid range\n",
    "        return actions\n",
    "\n",
    "   \n",
    "    # will use this to initialize both the critic network its slowly-changing target network with same structure\n",
    "    def generate_critic_network(self,trainable, reuse,mode=1):\n",
    "        if mode==1:\n",
    "            state_action = tf.concat([ANN.state_ph, ANN.action_ph], axis=1)\n",
    "        if mode==2:\n",
    "            state_action = tf.concat([ANN.state_ph,self.actor_net_value], axis=1)\n",
    "        if mode==3:\n",
    "            state_action = tf.concat([ANN.next_state_ph,self.target_actor_net_value], axis=1)\n",
    "        hidden = tf.layers.dense(state_action, h1_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        hidden_2 = tf.layers.dense(hidden, h2_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        hidden_3 = tf.layers.dense(hidden_2, h3_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        q_values = tf.layers.dense(hidden_3, 1, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        return q_values\n",
    "    def train_graph(self):\n",
    "        # One step TD targets y_i for (s,a) from experience replay\n",
    "        # = r_i + gamma*Q_slow(s',mu_slow(s')) if s' is not terminal\n",
    "        # = r_i if s' terminal\n",
    "        updated_q_values = tf.expand_dims(ANN.reward_ph, 1) + tf.expand_dims(ANN.is_not_terminal_ph, 1) * gamma * self.target_critic_net_value\n",
    "\n",
    "        # 1-step temporal difference errors\n",
    "        td_errors = updated_q_values - self.critic_net_value\n",
    "\n",
    "        # critic loss function (mean-square value error with regularization)\n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "        for var in self.critic_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                critic_loss += l2_reg_critic * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "        # critic optimizer\n",
    "        critic_train_op = tf.train.AdamOptimizer(lr_critic).minimize(critic_loss)\n",
    "\n",
    "        # actor loss function (mean Q-values under current policy with regularization)\n",
    "        actor_loss = -1*tf.reduce_mean(self.q_value_for_actor_net)\n",
    "        for var in self.actor_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                actor_loss += l2_reg_actor * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "        # actor optimizer\n",
    "        # the gradient of the mean Q-values wrt actor params is the deterministic policy gradient (keeping critic params fixed)\n",
    "        actor_train_op = tf.train.AdamOptimizer(lr_actor).minimize(actor_loss, var_list=self.actor_net_vars)\n",
    "        return actor_train_op,critic_train_op\n",
    "        \n",
    "    def update_wts_graph(self):\n",
    "            # update values for slowly-changing targets towards current actor and critic\n",
    "            update_slow_target_ops = []\n",
    "            for i, target_actor_var in enumerate(self.target_actor_net_vars):\n",
    "                update_slow_target_actor_op = target_actor_var.assign(tau*self.actor_net_vars[i]+(1-tau)*target_actor_var)\n",
    "                update_slow_target_ops.append(update_slow_target_actor_op)\n",
    "\n",
    "            for i, slow_target_var in enumerate(self.target_critic_net_vars):\n",
    "                update_slow_target_critic_op = slow_target_var.assign(tau*self.critic_net_vars[i]+(1-tau)*slow_target_var)\n",
    "                update_slow_target_ops.append(update_slow_target_critic_op)\n",
    "\n",
    "            update_slow_targets_op = tf.group(*update_slow_target_ops, name='update_slow_targets')\n",
    "            return update_slow_targets_op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=ANN()\n",
    "actor_net_value=Model.predict_graph()\n",
    "actor_train_op,critic_train_op=Model.train_graph()\n",
    "update_wts_op=Model.update_wts_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initialize session\n",
    "sess = tf.Session()\t\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0, Reward: -1647.251, Steps: 200, Final noise scale:   0.400\n",
      "Episode  1, Reward: -957.547, Steps: 200, Final noise scale:   0.396\n",
      "Episode  2, Reward: -1512.650, Steps: 200, Final noise scale:   0.392\n",
      "Episode  3, Reward: -1469.922, Steps: 200, Final noise scale:   0.388\n",
      "Episode  4, Reward: -1663.252, Steps: 200, Final noise scale:   0.384\n",
      "Episode  5, Reward: -1586.491, Steps: 200, Final noise scale:   0.380\n",
      "Episode  6, Reward: -1458.635, Steps: 200, Final noise scale:   0.377\n",
      "Episode  7, Reward: -1390.783, Steps: 200, Final noise scale:   0.373\n",
      "Episode  8, Reward: -1204.401, Steps: 200, Final noise scale:   0.369\n",
      "Episode  9, Reward: -1532.131, Steps: 200, Final noise scale:   0.365\n",
      "Episode 10, Reward: -1503.814, Steps: 200, Final noise scale:   0.362\n",
      "Episode 11, Reward: -1177.988, Steps: 200, Final noise scale:   0.358\n",
      "Episode 12, Reward: -1163.216, Steps: 200, Final noise scale:   0.355\n",
      "Episode 13, Reward: -905.482, Steps: 200, Final noise scale:   0.351\n",
      "Episode 14, Reward: -663.810, Steps: 200, Final noise scale:   0.347\n",
      "Episode 15, Reward: -773.686, Steps: 200, Final noise scale:   0.344\n",
      "Episode 16, Reward: -642.260, Steps: 200, Final noise scale:   0.341\n",
      "Episode 17, Reward: -753.074, Steps: 200, Final noise scale:   0.337\n",
      "Episode 18, Reward: -917.142, Steps: 200, Final noise scale:   0.334\n",
      "Episode 19, Reward: -679.666, Steps: 200, Final noise scale:   0.330\n",
      "Episode 20, Reward: -760.705, Steps: 200, Final noise scale:   0.327\n",
      "Episode 21, Reward: -770.238, Steps: 200, Final noise scale:   0.324\n",
      "Episode 22, Reward: -514.335, Steps: 200, Final noise scale:   0.321\n",
      "Episode 23, Reward: -641.810, Steps: 200, Final noise scale:   0.317\n",
      "Episode 24, Reward: -788.460, Steps: 200, Final noise scale:   0.314\n",
      "Episode 25, Reward: -230.004, Steps: 200, Final noise scale:   0.311\n",
      "Episode 26, Reward: -815.497, Steps: 200, Final noise scale:   0.308\n",
      "Episode 27, Reward: -134.380, Steps: 200, Final noise scale:   0.305\n",
      "Episode 28, Reward: -130.582, Steps: 200, Final noise scale:   0.302\n",
      "Episode 29, Reward: -482.876, Steps: 200, Final noise scale:   0.299\n",
      "Episode 30, Reward: -390.100, Steps: 200, Final noise scale:   0.296\n",
      "Episode 31, Reward: -303.078, Steps: 200, Final noise scale:   0.293\n",
      "Episode 32, Reward: -15.773, Steps: 200, Final noise scale:   0.290\n",
      "Episode 33, Reward: -131.280, Steps: 200, Final noise scale:   0.287\n",
      "Episode 34, Reward: -1502.606, Steps: 200, Final noise scale:   0.284\n",
      "Episode 35, Reward: -251.019, Steps: 200, Final noise scale:   0.281\n",
      "Episode 36, Reward: -247.501, Steps: 200, Final noise scale:   0.279\n",
      "Episode 37, Reward: -18.807, Steps: 200, Final noise scale:   0.276\n",
      "Episode 38, Reward: -270.156, Steps: 200, Final noise scale:   0.273\n",
      "Episode 39, Reward: -129.590, Steps: 200, Final noise scale:   0.270\n",
      "Episode 40, Reward: -256.170, Steps: 200, Final noise scale:   0.268\n",
      "Episode 41, Reward: -494.910, Steps: 200, Final noise scale:   0.265\n",
      "Episode 42, Reward: -137.336, Steps: 200, Final noise scale:   0.262\n",
      "Episode 43, Reward: -15.945, Steps: 200, Final noise scale:   0.260\n",
      "Episode 44, Reward: -148.981, Steps: 200, Final noise scale:   0.257\n",
      "Episode 45, Reward: -144.405, Steps: 200, Final noise scale:   0.254\n",
      "Episode 46, Reward: -270.074, Steps: 200, Final noise scale:   0.252\n",
      "Episode 47, Reward: -138.249, Steps: 200, Final noise scale:   0.249\n",
      "Episode 48, Reward: -13.985, Steps: 200, Final noise scale:   0.247\n",
      "Episode 49, Reward: -11.624, Steps: 200, Final noise scale:   0.244\n",
      "Episode 50, Reward: -240.024, Steps: 200, Final noise scale:   0.242\n",
      "Episode 51, Reward: -140.696, Steps: 200, Final noise scale:   0.240\n",
      "Episode 52, Reward: -139.977, Steps: 200, Final noise scale:   0.237\n",
      "Episode 53, Reward: -141.061, Steps: 200, Final noise scale:   0.235\n",
      "Episode 54, Reward: -144.653, Steps: 200, Final noise scale:   0.232\n",
      "Episode 55, Reward: -259.083, Steps: 200, Final noise scale:   0.230\n",
      "Episode 56, Reward: -142.788, Steps: 200, Final noise scale:   0.228\n",
      "Episode 57, Reward: -283.255, Steps: 200, Final noise scale:   0.226\n",
      "Episode 58, Reward: -136.669, Steps: 200, Final noise scale:   0.223\n",
      "Episode 59, Reward: -132.345, Steps: 200, Final noise scale:   0.221\n",
      "Episode 60, Reward: -138.961, Steps: 200, Final noise scale:   0.219\n",
      "Episode 61, Reward: -137.610, Steps: 200, Final noise scale:   0.217\n",
      "Episode 62, Reward: -134.056, Steps: 200, Final noise scale:   0.215\n",
      "Episode 63, Reward: -135.839, Steps: 200, Final noise scale:   0.212\n",
      "Episode 64, Reward: -128.337, Steps: 200, Final noise scale:   0.210\n",
      "Episode 65, Reward: -876.605, Steps: 200, Final noise scale:   0.208\n",
      "Episode 66, Reward: -16.263, Steps: 200, Final noise scale:   0.206\n",
      "Episode 67, Reward: -271.221, Steps: 200, Final noise scale:   0.204\n",
      "Episode 68, Reward: -241.198, Steps: 200, Final noise scale:   0.202\n",
      "Episode 69, Reward: -131.468, Steps: 200, Final noise scale:   0.200\n",
      "Episode 70, Reward: -136.471, Steps: 200, Final noise scale:   0.198\n",
      "Episode 71, Reward: -142.151, Steps: 200, Final noise scale:   0.196\n",
      "Episode 72, Reward: -236.214, Steps: 200, Final noise scale:   0.194\n",
      "Episode 73, Reward: -15.922, Steps: 200, Final noise scale:   0.192\n",
      "Episode 74, Reward: -126.969, Steps: 200, Final noise scale:   0.190\n",
      "Episode 75, Reward: -15.644, Steps: 200, Final noise scale:   0.188\n",
      "Episode 76, Reward: -128.050, Steps: 200, Final noise scale:   0.186\n",
      "Episode 77, Reward: -1651.998, Steps: 200, Final noise scale:   0.184\n",
      "Episode 78, Reward: -134.083, Steps: 200, Final noise scale:   0.183\n",
      "Episode 79, Reward: -142.015, Steps: 200, Final noise scale:   0.181\n",
      "Episode 80, Reward: -281.013, Steps: 200, Final noise scale:   0.179\n",
      "Episode 81, Reward: -17.494, Steps: 200, Final noise scale:   0.177\n",
      "Episode 82, Reward: -354.420, Steps: 200, Final noise scale:   0.175\n",
      "Episode 83, Reward: -142.030, Steps: 200, Final noise scale:   0.174\n",
      "Episode 84, Reward: -137.653, Steps: 200, Final noise scale:   0.172\n",
      "Episode 85, Reward: -137.096, Steps: 200, Final noise scale:   0.170\n",
      "Episode 86, Reward: -356.927, Steps: 200, Final noise scale:   0.169\n",
      "Episode 87, Reward: -135.949, Steps: 200, Final noise scale:   0.167\n",
      "Episode 88, Reward: -140.865, Steps: 200, Final noise scale:   0.165\n",
      "Episode 89, Reward: -134.447, Steps: 200, Final noise scale:   0.164\n",
      "Episode 90, Reward: -143.504, Steps: 200, Final noise scale:   0.162\n",
      "Episode 91, Reward: -137.723, Steps: 200, Final noise scale:   0.160\n",
      "Episode 92, Reward: -132.799, Steps: 200, Final noise scale:   0.159\n",
      "Episode 93, Reward: -141.270, Steps: 200, Final noise scale:   0.157\n",
      "Episode 94, Reward:  -7.149, Steps: 200, Final noise scale:   0.156\n",
      "Episode 95, Reward: -285.590, Steps: 200, Final noise scale:   0.154\n",
      "Episode 96, Reward: -128.935, Steps: 200, Final noise scale:   0.152\n",
      "Episode 97, Reward: -117.675, Steps: 200, Final noise scale:   0.151\n",
      "Episode 98, Reward: -119.323, Steps: 200, Final noise scale:   0.149\n",
      "Episode 99, Reward: -227.564, Steps: 200, Final noise scale:   0.148\n",
      "Episode 100, Reward: -124.044, Steps: 200, Final noise scale:   0.146\n",
      "Episode 101, Reward: -125.449, Steps: 200, Final noise scale:   0.145\n",
      "Episode 102, Reward: -245.821, Steps: 200, Final noise scale:   0.143\n",
      "Episode 103, Reward: -343.836, Steps: 200, Final noise scale:   0.142\n",
      "Episode 104, Reward: -239.801, Steps: 200, Final noise scale:   0.141\n",
      "Episode 105, Reward: -125.734, Steps: 200, Final noise scale:   0.139\n",
      "Episode 106, Reward: -122.537, Steps: 200, Final noise scale:   0.138\n",
      "Episode 107, Reward: -238.252, Steps: 200, Final noise scale:   0.136\n",
      "Episode 108, Reward: -257.773, Steps: 200, Final noise scale:   0.135\n",
      "Episode 109, Reward: -265.387, Steps: 200, Final noise scale:   0.134\n",
      "Episode 110, Reward: -1190.924, Steps: 200, Final noise scale:   0.132\n",
      "Episode 111, Reward: -149.928, Steps: 200, Final noise scale:   0.131\n",
      "Episode 112, Reward: -1477.175, Steps: 200, Final noise scale:   0.130\n",
      "Episode 113, Reward: -129.112, Steps: 200, Final noise scale:   0.128\n",
      "Episode 114, Reward: -1443.697, Steps: 200, Final noise scale:   0.127\n",
      "Episode 115, Reward: -366.509, Steps: 200, Final noise scale:   0.126\n",
      "Episode 116, Reward: -1599.277, Steps: 200, Final noise scale:   0.125\n",
      "Episode 117, Reward: -140.932, Steps: 200, Final noise scale:   0.123\n",
      "Episode 118, Reward: -1515.122, Steps: 200, Final noise scale:   0.122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 119, Reward: -1089.488, Steps: 200, Final noise scale:   0.121\n",
      "Episode 120, Reward: -251.479, Steps: 200, Final noise scale:   0.120\n",
      "Episode 121, Reward: -134.164, Steps: 200, Final noise scale:   0.119\n",
      "Episode 122, Reward: -138.877, Steps: 200, Final noise scale:   0.117\n",
      "Episode 123, Reward: -267.235, Steps: 200, Final noise scale:   0.116\n",
      "Episode 124, Reward: -133.958, Steps: 200, Final noise scale:   0.115\n",
      "Episode 125, Reward: -264.998, Steps: 200, Final noise scale:   0.114\n",
      "Episode 126, Reward: -136.956, Steps: 200, Final noise scale:   0.113\n",
      "Episode 127, Reward: -136.275, Steps: 200, Final noise scale:   0.112\n",
      "Episode 128, Reward: -131.482, Steps: 200, Final noise scale:   0.111\n",
      "Episode 129, Reward: -234.409, Steps: 200, Final noise scale:   0.109\n",
      "Episode 130, Reward: -10.195, Steps: 200, Final noise scale:   0.108\n",
      "Episode 131, Reward: -251.501, Steps: 200, Final noise scale:   0.107\n",
      "Episode 132, Reward: -133.757, Steps: 200, Final noise scale:   0.106\n",
      "Episode 133, Reward: -125.486, Steps: 200, Final noise scale:   0.105\n",
      "Episode 134, Reward: -255.482, Steps: 200, Final noise scale:   0.104\n",
      "Episode 135, Reward: -383.617, Steps: 200, Final noise scale:   0.103\n",
      "Episode 136, Reward: -126.300, Steps: 200, Final noise scale:   0.102\n",
      "Episode 137, Reward: -242.992, Steps: 200, Final noise scale:   0.101\n",
      "Episode 138, Reward: -248.362, Steps: 200, Final noise scale:   0.100\n",
      "Episode 139, Reward: -247.744, Steps: 200, Final noise scale:   0.099\n",
      "Episode 140, Reward: -244.242, Steps: 200, Final noise scale:   0.098\n",
      "Episode 141, Reward: -232.627, Steps: 200, Final noise scale:   0.097\n",
      "Episode 142, Reward: -399.575, Steps: 200, Final noise scale:   0.096\n",
      "Episode 143, Reward: -128.037, Steps: 200, Final noise scale:   0.095\n",
      "Episode 144, Reward: -126.566, Steps: 200, Final noise scale:   0.094\n",
      "Episode 145, Reward: -127.890, Steps: 200, Final noise scale:   0.093\n",
      "Episode 146, Reward:  -7.885, Steps: 200, Final noise scale:   0.092\n",
      "Episode 147, Reward: -126.153, Steps: 200, Final noise scale:   0.091\n",
      "Episode 148, Reward:  -8.845, Steps: 200, Final noise scale:   0.090\n",
      "Episode 149, Reward: -341.541, Steps: 200, Final noise scale:   0.089\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-422ec3a8b435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Finalize and upload results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mwritefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'info.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-16c3d45f0947>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(fname, s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwritefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'env_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m info['params'] = dict(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outdir' is not defined"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "## Training\n",
    "\n",
    "total_steps = 0\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "\ttotal_reward = 0\n",
    "\tsteps_in_ep = 0\n",
    "\n",
    "\t# Initialize exploration noise process\n",
    "\tnoise_process = np.zeros(action_dim)\n",
    "\tnoise_scale = (initial_noise_scale * noise_decay**ep) * (env.action_space.high - env.action_space.low)\n",
    "\n",
    "\t# Initial state\n",
    "\tobservation = env.reset()\n",
    "\t#if ep%10 == 0: env.render()\n",
    "    \n",
    "\tfor t in range(max_steps_ep):\n",
    "\n",
    "\t\t# choose action based on deterministic policy\n",
    "\t\taction_for_state, = sess.run(actor_net_value, \n",
    "\t\t\tfeed_dict = {Model.state_ph: observation[None]})\n",
    "\n",
    "\t\t# add temporally-correlated exploration noise to action (using an Ornstein-Uhlenbeck process)\n",
    "\t\t# print(action_for_state)\n",
    "\t\tnoise_process = exploration_theta*(exploration_mu - noise_process) + exploration_sigma*np.random.randn(action_dim)\n",
    "\t\t# print(noise_scale*noise_process)\n",
    "\t\taction_for_state += noise_scale*noise_process\n",
    "\n",
    "\t\t# take step\n",
    "\t\tnext_observation, reward, done, _info = env.step(action_for_state)\n",
    "\t\t#if ep%10 == 0: env.render()\n",
    "\t\ttotal_reward += reward\n",
    "\n",
    "\t\tadd_to_memory((observation, action_for_state, reward, next_observation, \n",
    "\t\t\t# is next_observation a terminal state?\n",
    "\t\t\t# 0.0 if done and not env.env._past_limit() else 1.0))\n",
    "\t\t\t0.0 if done else 1.0))\n",
    "\n",
    "\t\t# update network weights to fit a minibatch of experience\n",
    "\t\tif total_steps%train_every == 0 and len(replay_memory) >= minibatch_size:\n",
    "\n",
    "\t\t\t# grab N (s,a,r,s') tuples from replay memory\n",
    "\t\t\tminibatch = sample_from_memory(minibatch_size)\n",
    "\n",
    "\t\t\t# update the critic and actor params using mean-square value error and deterministic policy gradient, respectively\n",
    "\t\t\t_, _ = sess.run([critic_train_op, actor_train_op], \n",
    "\t\t\t\tfeed_dict = {\n",
    "\t\t\t\t\tModel.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "\t\t\t\t\tModel.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "\t\t\t\t\tModel.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "\t\t\t\t\tModel.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "\t\t\t\t\tModel.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch]),\n",
    "\t\t\t\t\t})\n",
    "\n",
    "\t\t\t# update slow actor and critic targets towards current actor and critic\n",
    "\t\t\t_ = sess.run(update_wts_op)\n",
    "\n",
    "\t\tobservation = next_observation\n",
    "\t\ttotal_steps += 1\n",
    "\t\tsteps_in_ep += 1\n",
    "\t\t\n",
    "\t\tif done: \n",
    "\t\t\t# Increment episode counter\n",
    "\t\t\t_ = sess.run(Model.episode_inc_op)\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\tprint('Episode %2i, Reward: %7.3f, Steps: %i, Final noise scale: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale))\n",
    "\n",
    "# Finalize and upload results\n",
    "env.close()\n",
    "gym.upload(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
